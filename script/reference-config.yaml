
sources:
  mysql:
    type: cdc
    logging:
      level: info
    connection:
      type: mysql
      host: <DB_HOST> # e.g. localhost
      port: 3306
      database: <DB_NAME> # e.g. inventory
      # user and password are injected from the secrets
      user: ${SOURCE_DB_USERNAME}
      password: ${SOURCE_DB_PASSWORD}
    #
    # Additional properties for the source collector:
    #
    # List of tables to be synced (optional)
    # tables:
    #   <SCHEMA_NAME>.<TABLE_NAME>:
    #     # List of columns to be synced (optional)
    #     columns:
    #       - <COLUMN_NAME>
    #       - <COLUMN_NAME>
    #     # List of columns to be used as keys (optional)
    #     keys:
    #       - <COLUMN_NAME>
    #
    # Example - Sync specific tables
    # tables:
    #   # Sync a specific table with all its columns:
    #   redislabscdc.account: {}
    #   # Sync a specific table with specific columns and keys:
    #   redislabscdc.emp:
    #     columns:
    #       - empno
    #       - fname
    #       - lname
    #     keys:
    #       - empno
    #
    # Advanced collector properties (optional):
    # advanced:
    #   # Sink collector properties - see the full list at https://debezium.io/documentation/reference/stable/operations/debezium-server.html#_redis_stream
    #   sink:
    #     redis.memory.limit.mb: 100
    #     redis.memory.threshold.percentage: 85
    #   # Source specific properties - see the full list at https://debezium.io/documentation/reference/stable/connectors/
    #   source:
    #     snapshot.mode: initial
    #   # Quarkus framework properties - see the full list at https://quarkus.io/guides/all-config
    #   quarkus:
    #     banner.enabled: "false"
    #
targets:
  # Redis target database connections
  # This section is for configuring connections to Redis databases to which Redis Data Integration will connect to
  # The default connection must be named 'target' and is used when no connection is specified in jobs or no jobs
  # are deployed. However multiple connections can be defined here and used in the job definition output blocks:
  # (e.g. target1, my-cloud-redis-db2, etc.)
  target:
    connection:
      type: redis
      # Host of the Redis database to which Redis Data Integration will write the processed data
      host: <REDIS_TARGET_DB_HOST> # e.g. localhost
      # Port for the Redis database to which Redis Data Integration will write the processed data
      port: <REDIS_TARGET_DB_PORT> # e.g. 12000
      # User of the Redis database to which Redis Data Integration will write the processed data. Uncomment if not using default user.
      # user: ${TARGET_DB_USERNAME}
      # Password for Redis target database
      password: ${TARGET_DB_PASSWORD}
      # uncomment the following lines if you are using SSL/TLS
      #key: ${TARGET_DB_KEY}
      #key_password: ${TARGET_DB_KEY_PASSWORD}
      #cert: ${TARGET_DB_CERT}
      #cacert: ${TARGET_DB_CACERT}

processors:
  # Interval (in seconds) on which to perform retry on failure
  #on_failed_retry_interval: 5
  # The batch size for reading data from source database
  #read_batch_size: 2000
  # Time (in ms) after which data will be read from stream even if read_batch_size was not reached
  #duration: 100
  # The batch size for writing data to target Redis database. Should be less or equal to the read_batch_size
  #write_batch_size: 200
  # Enable deduplication mechanism (default: false)
  #dedup: <DEDUP_ENABLED>
  # Max size of the deduplication set (default: 1024)
  #dedup_max_size: <DEDUP_MAX_SIZE>
  # Error handling strategy: ignore - skip, dlq - store rejected messages in a dead letter queue
  #error_handling: dlq
  # Dead letter queue max messages per stream
  #dlq_max_messages: 1000
  # Target data type: hash/json - RedisJSON module must be in use in the target DB
  #target_data_type: hash
  # Number of processes to use when syncing initial data
  #initial_sync_processes: 4
  # Checks if the batch has been written to the replica shard
  #wait_enabled: false
  # Timeout in milliseconds when checking write to the replica shard
  #wait_timeout: 1000
  # Ensures that a batch has been written to the replica shard and keeps retrying if not
  #retry_on_replica_failure: true
  # Enable Debezium LOB placeholders for tables that contain large binary objects
  #debezium_lob_encoded_placeholder: X19kZWJleml1bV91bmF2YWlsYWJsZV92YWx1ZQ==
  # Enable merge as the default strategy to writing JSON documents
  #json_update_strategy: merge